"""Baseline w History Benchmark.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pTfz8L-wnBPl1yEL1uAcj-REFCX_OtSD

According to the "Counting to Explore and Generalize in Text-based Games" paper, using history is the same thing as appending the previous 4 steps' history observation into the input.
"""

'''
Train on a single fixed game seed for Level 10.
In between each "epoch" (i.e. each episode), test on 4 games with different fixed seeds.
For each episode, look at training loss and test performance.
After training agent for N epochs, show agent's performance on a single seed game from harder levels (L30, L50, L70).
'''

import matplotlib as mpl
mpl.use('TkAgg')

import random
import torch
import numpy as np
import textworld.gym
from typing import Mapping, Any
import os
import subprocess
import matplotlib.pyplot as plt

import nltk
nltk.download('punkt')
from nltk import word_tokenize

TRAIN_SEED = 0
TEST_SEEDS = [74, 10, 91, 77]
TRAIN_LEVEL = 10
REWARD_SCALE = 1000
NUM_EPOCHS = 1


print('TRAIN_SEED', TRAIN_SEED)
print('TEST_SEEDS', TEST_SEEDS)

def make_game(seed, level):
  output = f'tw_games/coin_collector_lev{level}_seed{seed}_game.ulx'
  if not os.path.exists(output):
    command = ['tw-make', 'tw-coin_collector', '--level', str(level), '--seed', str(seed), '--output', output]
    subprocess.run(command)

  request_infos = textworld.EnvInfos(
    admissible_commands=True,  # All commands relevant to the current state.
    entities=True,              # List of all interactable entities found in the game.
    facts=True
  )

  env_id = textworld.gym.register_game(output, request_infos)
  return env_id

TRAIN_ENV = make_game(TRAIN_SEED, TRAIN_LEVEL)
TEST_ENVS = [make_game(seed, TRAIN_LEVEL) for seed in TEST_SEEDS]

print('TRAIN_ENV ID', TRAIN_ENV)
print('TEST_ENVS IDS', TEST_ENVS)

"""# 1.&nbsp;LSTM-DQN Agent"""

class LSTM_DQN(torch.nn.Module):
  def __init__(self, emb_size, action_space, hidden_dim):
    super(LSTM_DQN, self).__init__()

    self.emb = torch.nn.Embedding(emb_size, hidden_dim)
    self.lstm = torch.nn.LSTM(hidden_dim, hidden_dim, num_layers=1)
    self.lstm_hidden_clean = (torch.randn(1, hidden_dim), torch.randn(1, hidden_dim))

    self.dqn = torch.nn.Sequential(
        torch.nn.Linear(emb_size, hidden_dim),
        torch.nn.ReLU(),
        torch.nn.Linear(hidden_dim, action_space)
    )

  def forward(self, input_tensor):
    embeddings = self.emb(input_tensor)
    hidden = self.lstm_hidden_clean
    output, hidden = self.lstm(embeddings, hidden)
    state_rep = torch.mean(output, dim=1)
    q_vals = self.dqn(state_rep)
    return q_vals

class LSTM_DQN_Agent():
  def __init__(self, vocab_size, max_num_actions):
    self.word_indices = {'UNK': 0}
    self.last_word_idx = 1
    self.action_indices = {'UNK': 0}
    self.idx_to_action = {0: 'UNK'}
    self.last_action_idx = 1

    self.model = LSTM_DQN(emb_size=vocab_size+max_num_actions, action_space=max_num_actions, hidden_dim=128)
    self.optimizer = torch.optim.Adam(self.model.parameters(), 0.003)
    self.vocab_size = vocab_size
    self.num_actions = max_num_actions
    self.loss_fcn = torch.nn.MSELoss()

  def vectorize_obs(self, tokens, eval_mode):
    for token in tokens:
      if not eval_mode and token not in self.word_indices.keys():
        self.word_indices[token] = self.last_word_idx
        self.last_word_idx+=1

    if eval_mode:
      word_vec = [self.word_indices[token] if token in self.word_indices else 0 for token in tokens]
    else:
      word_vec = [self.word_indices[token] for token in tokens if token in self.word_indices]

    torch_indices = torch.tensor(word_vec)

    one_hot = torch.nn.functional.one_hot(torch_indices, num_classes=self.vocab_size)
    token_vector = torch.sum(one_hot, dim=0)

    return token_vector

  def tokenize_obs(self, sentence):
    tokens = word_tokenize(sentence)
    return tokens

  def process_obs(self, sentence, eval_mode=False):
    tokens = self.tokenize_obs(sentence)
    vectors = self.vectorize_obs(tokens, eval_mode)
    return vectors

  def process_actions(self, actions, eval_mode=False):
    for action in actions:
      if not eval_mode and action not in self.action_indices.keys():
        self.action_indices[action] = self.last_action_idx
        self.idx_to_action[self.last_action_idx] = action
        self.last_action_idx+=1

    if eval_mode:
      action_vec = [self.action_indices[action] if action in self.action_indices else 0 for action in actions]
    else:
      action_vec = [self.action_indices[action] for action in actions if action in self.action_indices]

    torch_indices = torch.tensor(action_vec)

    one_hot = torch.full((self.num_actions, ), 0)
    one_hot[torch_indices] = 100

    return one_hot

  def get_qvals(self, obs, infos, reward, eval_mode=False):
    obs_tensor = self.process_obs(obs, eval_mode)
    actions_tensor = self.process_actions(infos['admissible_commands'], eval_mode)

    input_tensor = torch.cat((obs_tensor,actions_tensor), 0)

    if eval_mode:
      self.model.eval()
      with torch.no_grad():
        q_vals = self.model(input_tensor)
      self.model.train()
    else:
      q_vals = self.model(input_tensor)

    return q_vals

  def get_max_qval(self, q_vals, infos):
    max_qval = -np.inf
    best_action = infos['admissible_commands'][0]
    for action in infos['admissible_commands']:
      if action == "look" or action == "inventory":
        continue
      idx = self.action_indices.get(action, 0)
      if q_vals[idx] > max_qval:
        max_qval = q_vals[idx]
        best_action = action

    return (best_action, max_qval)


  def act(self, obs, infos, reward, q_vals, eps_greedy=True):
    if eps_greedy:
      random = np.random.random()
      if random < 1.0/len(infos['admissible_commands']):
        idx = np.random.randint(len(infos['admissible_commands']))
        action = infos['admissible_commands'][np.random.randint(len(infos['admissible_commands']))]
        q_idx = self.action_indices[action]
        return action, q_vals[q_idx]


    action, max_val = self.get_max_qval(q_vals, infos)

    return action, max_val



def train(agent, env_id, max_num_moves, gamma=0.9):
  num_episodes = NUM_EPOCHS
  losses = []

  for ep in range(num_episodes):
    env = textworld.gym.make(env_id)
    obs, infos = env.reset()

    done = False
    obs, score, done, infos = env.step('look')
    num_moves = 0

    selected_qvals = []
    target_qvals = []

    history_obs = ['', '', '', '', obs]

    while num_moves < max_num_moves and not done:
      full_obs = ' '.join(history_obs)

      q_vals = agent.get_qvals(full_obs, infos, score)
      action, action_qval = agent.act(full_obs, infos, score, q_vals, True)

      obs, score, done, infos = env.step(action)
      history_obs.pop(0)
      history_obs.append(obs)

      score *= REWARD_SCALE

      next_qvals = agent.get_qvals(full_obs, infos, score)
      next_action, next_action_qval = agent.get_max_qval(next_qvals, infos)

      selected_qvals.append(action_qval)
      target_qvals.append(score + gamma * next_action_qval)

    loss = agent.loss_fcn(torch.stack(selected_qvals), torch.stack(target_qvals))
    loss.backward()

    agent.optimizer.step()
    agent.optimizer.zero_grad()

    losses.append(loss.detach().item())
    print('LOSS:', losses[-1])

    all_test_scores, all_num_moves = test_on_train_level(agent, max_num_moves)
    plot_test_on_train(all_test_scores, all_num_moves, max_num_moves)

  return agent, losses

# Each game was generated with a fixed seed, so the game dynamics do not change
# At test time, we make the agent greedy w.r.t. to the learned value function, so the policy used is also deterministic
# Accordingly, it's sufficient to only collect stats for one run of the agent on each game because the results can't change across runs

def test_on_train_level(agent, max_num_moves):
  all_test_envs = [TRAIN_ENV] + TEST_ENVS

  all_test_scores = [0] * len(all_test_envs)
  all_num_moves = [0] * len(all_test_envs)

  for i, env_id in enumerate(all_test_envs):
    env = textworld.gym.make(env_id)
    obs, infos = env.reset()

    done = False
    obs, score, done, infos = env.step('look')
    num_moves = 0

    history_obs = ['', '', '', '', obs]

    while num_moves < max_num_moves and not done:
      full_obs = ' '.join(history_obs)
      q_vals = agent.get_qvals(full_obs, infos, score, eval_mode=True)
      action, action_q_val = agent.act(full_obs, infos, score, q_vals, False)

      obs, score, done, infos = env.step(action)
      history_obs.pop(0)
      history_obs.append(obs)

      score *= REWARD_SCALE

      num_moves += 1

    all_test_scores[i] = score
    all_num_moves[i] = max_num_moves

  print('TEST SCORES:', all_test_scores)
  print('MOVES MADE BEFORE END OF EPISODE', all_num_moves)

  return all_test_scores, all_num_moves

def plot_test_on_train(all_test_scores, all_num_moves, max_num_moves):
  all_test_seeds = [TRAIN_SEED] + TEST_SEEDS
  games = [str(seed) for seed in all_test_seeds]

  fig, axs = plt.subplots(1, 2, figsize=(12,5))
  axs[0].bar(games, all_test_scores)
  axs[0].set_title('Scores for 1 Play of Each Test Game [Level 10]')
  axs[0].set_xlabel('Game Seed')
  axs[0].set_ylabel('Game Score')

  axs[1].bar(games, all_num_moves)
  axs[1].set_title('# Moves for 1 Play of Each Test Game [Level 10]')
  axs[1].set_xlabel('Game Seed')
  axs[1].set_ylabel(f'# Moves out of {max_num_moves}')

  plt.tight_layout()
  plt.show()

# Let's test on an untrained agent just for fun
untrained_baseline = LSTM_DQN_Agent(vocab_size=10000, max_num_actions=1000)
quest_length = ((TRAIN_LEVEL - 1) % 100 + 1)
max_num_moves = quest_length * 2
all_test_scores, all_num_moves = test_on_train_level(untrained_baseline, max_num_moves)
plot_test_on_train(all_test_scores, all_num_moves, max_num_moves)

# Let's train the agent now
untrained_baseline = LSTM_DQN_Agent(vocab_size=10000, max_num_actions=1000)
trained_baseline = train(untrained_baseline, TRAIN_ENV, max_num_moves)
trained_agent, losses = trained_baseline

def plot_loss(losses, num_epochs):
  epochs = list(range(0, num_epochs))
  plt.plot(epochs, losses)
  plt.title('Training Loss')
  plt.xlabel('Epoch')
  plt.ylabel('Batched Loss')

  plt.grid(True)
  plt.show()

plot_loss(losses, 1)